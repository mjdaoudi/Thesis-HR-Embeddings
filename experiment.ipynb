{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(tokenizer, max_features, vector_dimension, model_embed, word2vec):\n",
    "    \"\"\"\n",
    "    A method to create the embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_matrix = np.zeros((max_features + 1, vector_dimension))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index > max_features:\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                if word2vec :\n",
    "                    embedding_matrix[index] = model_embed.wv[word]\n",
    "                else : \n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "            except:\n",
    "                continue\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_tensor(string_list: list, tokenizer, max_len) -> list:\n",
    "    \"\"\"\n",
    "    A method to convert a string list to a tensor for a deep learning model\n",
    "    \"\"\"    \n",
    "    string_list = tokenizer.texts_to_sequences(string_list)\n",
    "    string_list = pad_sequences(string_list, maxlen=max_len)\n",
    "    \n",
    "    return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = pd.read_pickle(\"data/jobDescription4.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc[\"labels\"] = np.where(desc[\"board\"]==\"web\", 1, 0)\n",
    "desc = desc[[\"Description_processed\", \"labels\"]].sample(frac = 1).head(500)\n",
    "\n",
    "desc[\"Description_processed\"] = desc[\"Description_processed\"].apply(lambda x: clean_text(x)) \n",
    "\n",
    "desc.to_pickle(\"experiment/exp.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "data = pd.read_pickle('experiment/train.pkl')\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "#c_word2vec = gensim.models.Word2Vec.load(\"models/custom/Word2Vec.model\")\n",
    "#c_fasttext = gensim.models.Word2Vec.load(\"models/custom/FastText.model\")\n",
    "#c_glove = gensim.models.KeyedVectors.load_word2vec_format(\"models/custom/GloVe.txt\")\n",
    "#glove = gensim.models.KeyedVectors.load_word2vec_format(\"models/pre-trained/wvglove.6B.100d.txt\")\n",
    "\n",
    "\n",
    "#glove300 = gensim.models.KeyedVectors.load_word2vec_format(\"models/pre-trained/wvglove.42B.300d.txt\")\n",
    "#c_glove300 = gensim.models.KeyedVectors.load_word2vec_format(\"models/custom/GloVe_300d.txt\")\n",
    "c_word2vec300 = gensim.models.Word2Vec.load(\"models/custom/Word2Vec_300d.model\")\n",
    "c_fasttext300 = gensim.models.Word2Vec.load(\"models/custom/FastText_300d.model\")\n",
    "\n",
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pandas', 0.6743684411048889),\n",
       " ('pyspark', 0.6333119869232178),\n",
       " ('scala', 0.6253077983856201),\n",
       " ('flask', 0.6234632730484009),\n",
       " ('sqlalchemy', 0.6211127042770386),\n",
       " ('numpy', 0.6177347302436829),\n",
       " ('tensorflow', 0.61696457862854),\n",
       " ('keras', 0.6124801635742188),\n",
       " ('ggplot', 0.605608344078064),\n",
       " ('matlab', 0.5970077514648438)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_word2vec300.wv.most_similar(\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout, GlobalAveragePooling1D, Bidirectional, InputLayer\n",
    "from tensorflow import keras\n",
    "\n",
    "def RnnModel(embedding_matrix, embedding_dim, max_len):\n",
    "    inp1 = Input(shape=(max_len,))\n",
    "    x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(150))(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)    \n",
    "    model = Model(inputs=inp1, outputs=x)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "    return model\n",
    "\n",
    "def SimpleModel(embedding_matrix, embedding_dim, max_len):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix]))\n",
    "    #model.add(keras.layers.Embedding(embedding_matrix.shape[0], embedding_dim))\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(16, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32470, 300)\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 300)         9741000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_5 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 9,745,833\n",
      "Trainable params: 9,745,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(34152, 300)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         10245600  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 10,250,433\n",
      "Trainable params: 10,250,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(34397, 300)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 300)         10319100  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 10,323,933\n",
      "Trainable params: 10,323,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n    callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\\n\\n    batch_size = 32\\n    epochs = 15\\n\\n    model.fit(\\n        X_train,\\n        Y_train, \\n        batch_size=batch_size, \\n        epochs=epochs,\\n        callbacks=[callback]\\n    )\\n\\n    X_test = validation_data['Description_processed'].tolist()\\n    Y_test = np.asarray(validation_data['labels'].tolist())\\n    \\n    X_test = [clean_text(text) for text in X_test]\\n    X_test = string_to_tensor(X_test, tokenizer, max_len)\\n    yhat = [x[0] for x in model.predict(X_test).tolist()]\\n\\n    acc = accuracy_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])\\n    accuracies.append(acc)\\n    keras.backend.clear_session()\\n\\nprint(accuracies)\\nmean(accuracies)\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracies = []\n",
    "for train_index, val_index in kf.split(data):\n",
    "    training_data = data.iloc[train_index]\n",
    "    validation_data = data.iloc[val_index]\n",
    "\n",
    "    X_train = training_data['Description_processed'].tolist()\n",
    "    Y_train = np.asarray(training_data['labels'].tolist())\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    emb_matrix = create_embedding_matrix(tokenizer, len(tokenizer.word_counts), emb_dim, c_fasttext300, False)\n",
    "    print(emb_matrix.shape)\n",
    "        \n",
    "    max_len = np.max([len(text.split()) for text in X_train])\n",
    "    X_train = string_to_tensor(X_train, tokenizer, max_len)\n",
    "\n",
    "    model = SimpleModel(emb_matrix, emb_dim, max_len)\n",
    "    print(model.summary())\n",
    "\"\"\"\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 15\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        Y_train, \n",
    "        batch_size=batch_size, \n",
    "        epochs=epochs,\n",
    "        callbacks=[callback]\n",
    "    )\n",
    "\n",
    "    X_test = validation_data['Description_processed'].tolist()\n",
    "    Y_test = np.asarray(validation_data['labels'].tolist())\n",
    "    \n",
    "    X_test = [clean_text(text) for text in X_test]\n",
    "    X_test = string_to_tensor(X_test, tokenizer, max_len)\n",
    "    yhat = [x[0] for x in model.predict(X_test).tolist()]\n",
    "\n",
    "    acc = accuracy_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])\n",
    "    accuracies.append(acc)\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "print(accuracies)\n",
    "mean(accuracies)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 300)         10319100  \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                4816      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 10,323,933\n",
      "Trainable params: 10,323,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b75524168a7629365e64c8aaac988d7faff873bd2e18d7e50a62762b40834f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
